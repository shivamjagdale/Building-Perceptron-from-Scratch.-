{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-R0O2YAx8Ka3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SigmoidPerceptroon():\n",
        "\n",
        "  def __init__(self, input_size ):\n",
        "    # input_size is nothing but the shape of your data no of columns you have in your dataset.\n",
        "\n",
        "    self.weights = np.random.rand(input_size)\n",
        "    # Here the weights are nothing but the weights of the input features.\n",
        "    # Rememeber the no of weights you have in a model is equal to the no of input features you have. We don't know how many columns our input feature has so we do that\n",
        "    # The user can mention the shape of the data. It will initiate a weight array or weight vector which contains same no of values as no of columns in our data.\n",
        "\n",
        "    self.bias = np.random.rand(1)\n",
        "    # Here we are giving this as 1 because we know bias is a single scaler value. Here it is one so it will generate a single value.\n",
        "\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    # So we know we apply sigmoid function to the weighted sum so we call that as a z.\n",
        "\n",
        "    return 1/(1+np.exp(-z))\n",
        "    # That is the sigmoid function. That is the actual output we are sending from the model. The z will be calculated in the predict function.\n",
        "    # Z is that w1*x1 + w2*x2 and we will pass this Z to the sigmoid activation function.\n",
        "\n",
        "  def predict(self, inputs):\n",
        "    # To predict it we need to give input values. This input we need to pass while calling the predict function.\n",
        "\n",
        "    weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "    # here we are finding a dot product of a vector. If you multiply or do a cross product you won't get a scalar value but for dot product you get a scalar value.\n",
        "    # We know input and weight will be in a form of a vector. input = [10,20,30] weight = [0.5,0.2,0.3] when you call dot it multiplies that 10 with 0.5\n",
        "    # So what happens is we have a weight that is randomly generated and a bias that is randomly generated, to that you pass the input and then it will calculate the weighted\n",
        "    # sum and then it will call the sigmoid function which gives us the output.\n",
        "\n",
        "    return self.sigmoid(weighted_sum)\n",
        "    # Here we are calling the sigmoid function using self.sigmoid and passing the weighed_sum\n",
        "\n",
        "  def fit(self, inputs, targets, learning_rate, epochs):\n",
        "    # Here in fit function we optimize the model. Update the weights and get the final optimized weight.\n",
        "    # Meaning when you use that particular weight we would get the best prediction.\n",
        "    # We keep updating this weight and bias until we reach a point where the accuracy cannot go further up. Or we can't reduce a loss value beyond that.\n",
        "\n",
        "    # Targets are nothing but labels, we know learning rate, epochs\n",
        "\n",
        "    num_examples = inputs.shape[0]\n",
        "    # This num_examples is nothing but no of datapoints that we have\n",
        "\n",
        "    for epoch in range(epochs): # This for loop runs for whatever number you mentioned in epochs.\n",
        "\n",
        "      for i in range(num_examples):\n",
        "        # We are using stochatist gradient descent where we will update the weights and parameters while training for each individual datapoints.\n",
        "        # If you are using a normal gradient descent you wont have this for loop here. The only for loop you would have is the epoch one.\n",
        "        # When we use SGD at each epoch we have another for loop that iterates over each individual datapoints. It will take one particular datapoint, predict the label\n",
        "        # and compare that datapoint to that label.\n",
        "\n",
        "        input_vector = inputs[i]  # Takes 1st row alone\n",
        "        target = targets[i]\n",
        "\n",
        "        prediction = self.predict(input_vector)\n",
        "        # This self.predict parameter takes input as a parameters so that is nothing but the input_vector\n",
        "\n",
        "        error = target-prediction\n",
        "\n",
        "        # Update weights\n",
        "        gradient_weights = error * prediction * (1-prediction) * input_vector # dw\n",
        "        self.weights = learning_rate*gradient_weights\n",
        "\n",
        "        # Update bias\n",
        "        gradient_bias = error * prediction * (1-prediction) # db\n",
        "        self.bias = learning_rate*gradient_bias\n",
        "\n",
        "  def evaluate(self, inputs, targets):\n",
        "\n",
        "    correct = 0\n",
        "    # Just using this as a counter to find how many correct values our model is predicting.\n",
        "    # We run this evaluation after the fit function.\n",
        "\n",
        "    for input_vector, target in zip(inputs, targets): # Look below for more info on this\n",
        "      prediction = self.predict(input_vector)\n",
        "\n",
        "      if prediction >=0.5:\n",
        "        predicted_class=1\n",
        "      else:\n",
        "        predicted_class=0\n",
        "\n",
        "      if predicted_class==target:\n",
        "        correct+=1\n",
        "\n",
        "      accuracy = correct/len(inputs)\n",
        "\n",
        "      return accuracy\n"
      ],
      "metadata": {
        "id": "IJcUAfVT8Mlx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = [10,20,30]\n",
        "list2 = [40,50,60]\n",
        "\n",
        "for a in list1:\n",
        "  print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl9luQGz_SEf",
        "outputId": "c0bf6ec3-4c0e-4b68-ca93-109eb5b0bd76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "20\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = [10,20,30]\n",
        "list2 = [40,50,60]\n",
        "\n",
        "for a,b in zip(list1, list2):\n",
        "  print(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs8DWrElfNKQ",
        "outputId": "a1314f1e-eee9-4fb6-e2af-ab4cb7999f57"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 40\n",
            "20 50\n",
            "30 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RlExvLh7gjOu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}